# 一图读懂apollo-cyber中的消息传递 #

**apollo-shm**

![image-20221215233741882](.assets/image-20221215233741882.png)

apollo中的消息传递按照线程间(指针)，进程间(shm)，主机间(fastrtps实现的dds)分为三类。这里分析下比较常见的进程间通信。

# Reader的创建 #

同一进程可以有多个节点(node)，这些节点可以监听(reader)相同的话题(channel)，话题的个数(msg)也没有限制(4个)。

在reader的`init`方法中，传入了自己的回调，回调中的`BlockerPtr::Enqueue`先不看，`reader_func_`就是我们指定的回调函数，在这里写收到信息后执行的业务逻辑。

```cpp
std::function<void(const std::shared_ptr<MessageT>&)> func;
if (reader_func_ != nullptr) {
  func = [this](const std::shared_ptr<MessageT>& msg) {
    this->Enqueue(msg);
    this->reader_func_(msg);
  };
} else {
  func = [this](const std::shared_ptr<MessageT>& msg) { this->Enqueue(msg); };
}
```

后面创建了一个`data::DataVisitor<MessageT>`数据监视器。可以看到DataVisitor有四种模板类型，可以同时监视1-4种类型的数据。构造时会开辟一块由用户指定大小的`BufferType<T>(CacheBuffer)`和channel_id绑定。同时这块buffer也被注册进了`DataDispatcher<M0>`中，之后一旦channel有数据，就会将数据(智能指针)Fill入这块Buffer。可以看到红色和蓝色的Reader虽然监视相同的话题，但是对应的并不是同一块buffer，因为buffer的size可以由用户指定，所以并没有使用同一块内存，不过存放智能指针的开销也很小。dv还有一个`TryFetch`方法，这个方法之后会在协程中调用，尝试取一次buffer中的数据。

```cpp
// init data visitor
buffer_(configs.channel_id, new BufferType<M0>(configs.queue_size)
DataDispatcher<M0>::Instance()->AddBuffer(buffer_);
```

协程的创建，绑定了dv和回调函数，放入调度中待执行

```cpp
// Using factory to wrap templates.
croutine::RoutineFactory factory =
    croutine::CreateRoutineFactory<MessageT>(std::move(func), dv);
if (!sched->CreateTask(factory, croutine_name_)) {
```

`CreateRoutineFactory`可以看到协程的内部执行的内容就是，设置协程状态为**DATA_WAIT**。尝试取一次数据，取到及回调到用户函数，否则Yield。再后面可以看到调度器只会执行状态为**Ready**的协程，所以在Reader创建好后会执行一次TryFetch函数，如果此时没有数据，协程的状态就是DATA_WAIT，之后除非等到数据到来，否则调度器不会再执行该协程。不会消耗再无意义的遍历Ready协程上。

```cpp
template <typename M0, typename F>//协程在 Scheduler::CreateTask 中创建
RoutineFactory CreateRoutineFactory(
    F&& f, const std::shared_ptr<data::DataVisitor<M0>>& dv) {
  RoutineFactory factory;
  factory.SetDataVisitor(dv);
  factory.create_routine = [=]() {
    return [=]() {
      std::shared_ptr<M0> msg;
      for (;;) {//死循环中的协程，没关系，会Yeild不会阻塞
        CRoutine::GetCurrentRoutine()->set_state(RoutineState::DATA_WAIT);
        if (dv->TryFetch(msg)) {
          f(msg);//用户回调
          CRoutine::Yield(RoutineState::READY);
        } else {
          CRoutine::Yield();
        }
      }
    };
  };
  return factory;
}
```

这里的Receiver是针对消息类型的单例，由ReceiverManager管理。实际是`HybridReceiver`但这里只用到了**ShmReceiver**。又注册了AddListener回调，内容是执行`OnNewMessage`，`OnNewMessage`会执行到Transport::Instance()->CreateReceiver中的回调函数。

```cpp
template <typename M>
void ShmReceiver<M>::Enable() {
  if (this->enabled_) {
    return;
  }

  dispatcher_->AddListener<M>(
      this->attr_, std::bind(&ShmReceiver<M>::OnNewMessage, this,
                             std::placeholders::_1, std::placeholders::_2));
  this->enabled_ = true;
}

//OnNewMessage
void Receiver<M>::OnNewMessage(const MessagePtr& msg,
                               const MessageInfo& msg_info) {
  if (msg_listener_ != nullptr) {
    msg_listener_(msg, msg_info, attr_);// 回调到Receiver构造时传入的回调函数
      // 即Transport::Instance()->CreateReceiver中的回调函数
  }
}
```

Transport::CreateReceiver的回调为`data::DataDispatcher<MessageT>::Instance()->Dispatch`处进行数据派发

总体来说，Receiver作用是，监视某个channel的变化，当对端数据写入共享内存后，会通过层层回调调用对应消息类型的DataDispatcher进行**数据的派发**。

```cpp
receiver_ = ReceiverManager<MessageT>::Instance()->GetReceiver(role_attr_);

receiver_map_[channel_name] =
    transport::Transport::Instance()->CreateReceiver<MessageT>(
        role_attr, [](const std::shared_ptr<MessageT>& msg,
                      const transport::MessageInfo& msg_info,
                      const proto::RoleAttributes& reader_attr) {
          data::DataDispatcher<MessageT>::Instance()->Dispatch(reader_attr.channel_id(), msg);
        });
```

# 消息的传递 #

再写节点创立后，Writer往共享内存中写入数据后。会被`ShmDispatcher::ThreadFunc`中的**Listen**捕获到`readable_info`。首先检查channel_id是不是本进程关注的。之后调用`ShmDispatcher::ReadMessage` ->`ShmDispatcher::OnMessage`,回调到`ShmDispatcher::AddListener` 中绑定的 listener_adapter。图中可以看出，`readable_info`只是声明了channel在内存块的某个idx写入数据，具体数据内容还是要根据共享内存的地址取实际内容。

```cpp
auto listener_adapter = [listener](const std::shared_ptr<ReadableBlock>& rb,
                                     const MessageInfo& msg_info) {
    auto msg = std::make_shared<MessageT>();
    RETURN_IF(!message::ParseFromArray(
        rb->buf, static_cast<int>(rb->block->msg_size()), msg.get())); 
    //在这里反序列化msg数据，msg_info的反序列化在ShmDispatcher::ReadMessage中完成
    listener(msg, msg_info); 
    // 回调到在ShmReceiver<M>::Enable() 中 注册的 ShmReceiver<M>::OnNewMessage 即 Receiver<M>::OnNewMessage
  };
```

回调到了`Receiver<M>::OnNewMessage`，上文中说到过OnNewMessage对应的动作就是DataDispatcher进行**数据的派发**

```cpp
//派发消息，派发完毕后Notify
template <typename T>
bool DataDispatcher<T>::Dispatch(const uint64_t channel_id,
                                 const std::shared_ptr<T>& msg) {
  BufferVector* buffers = nullptr;
  if (apollo::cyber::IsShutdown()) {
    return false;
  }
  if (buffers_map_.Get(channel_id, &buffers)) {
    for (auto& buffer_wptr : *buffers) {
      if (auto buffer = buffer_wptr.lock()) {
        std::lock_guard<std::mutex> lock(buffer->Mutex());
        buffer->Fill(msg);//
      }
    }
  } else {
    return false;
  }
  return notifier_->Notify(channel_id);
  //after Fill channel_id的消息来了，通知到监听对应channel的notifiers，依次执行notifier的cb
}
```

再**派发数据完毕**后，通知到监听对应channel的notifiers，依次执行notifier的cb。

```cpp
//channel_id的消息来了，通知到监听对应channel的notifiers，依次执行notifier的cb
inline bool DataNotifier::Notify(const uint64_t channel_id) {
  NotifyVector* notifies = nullptr;
  if (notifies_map_.Get(channel_id, &notifies)) {
    for (auto& notifier : *notifies) {
      if (notifier->callback) {
        notifier->callback();
      }
    }
    return true;
  }
  return false;
}
```

这个cb是在`Scheduler::CreateTask`中**生成CRoutine**创建的，**通知处理器执行该协程**。

```cpp
if (visitor != nullptr) {
  visitor->RegisterNotifyCallback([this, task_id, name]() {
    if (unlikely(stop_.load())) {
      return;
    }
    this->NotifyProcessor(task_id);// 协程状态设置就绪，notify processor 取协程任务执行
  });
}
```

最终数据来临时，被唤醒的协程TryFetch一定成功，执行到了用户的回调。这种情况下用户回调在不同的协程中执行，如果不在同一个执行器中，那么就不会有阻塞等待问题。你甚至可以在自己的读回调中sleep...

# 消息传递过程中的3个index

## 1 Indicator中的idx： next_listen_num_和written_info_num

![image-20221008214815597](.assets/image-20221008214815597.png)

```cpp
class ConditionNotifier : public NotifierBase {
  struct Indicator {
    std::mutex mtx;
    std::condition_variable cv;
    uint64_t written_info_num = 0;
    ReadableInfo infos[kBufLength];
  };

 public:
  virtual ~ConditionNotifier();

  void Shutdown() override;
  bool Notify(const ReadableInfo& info) override;
  bool Listen(int timeout_ms, ReadableInfo* info) override;

  static const char* Type() { return "condition"; }

 private:
  bool Init();
  bool OpenOrCreate();
  bool OpenOnly();
  bool Remove();
  void Reset();

  key_t key_ = 0;
  void* managed_shm_ = nullptr;
  size_t shm_size_ = 0;
  Indicator* indicator_ = nullptr;
  uint64_t next_listen_num_ = 0;
  std::atomic<bool> is_shutdown_ = {false};

  DECLARE_SINGLETON(ConditionNotifier)
};
```

Indicator 只有一个，对应一块共享内存，是一块kBufLength(4096)长度的ReadableInfo数组。该共享内存用于进程同步的方式是**条件变量和互斥锁**，**PTHREAD_PROCESS_SHARED**(进程间同步)。written_info_num是写节点写完数据后调用 Notify 增加，环形的数组。每次写的是 **host** 往 **channel** 的 **block_idx** 中添加了一条新数据

```cpp
bool ConditionNotifier::Notify(const ReadableInfo& info) {
  if (is_shutdown_.load()) {
    ADEBUG << "notifier is shutdown.";
    return false;
  }

  {
    std::unique_lock<std::mutex> lck(indicator_->mtx);
    auto idx = indicator_->written_info_num % kBufLength;
    indicator_->infos[idx] = info;
    ++indicator_->written_info_num;
  }

  indicator_->cv.notify_all();

  return true;
}
```

每个进程中的 ConditionNotifier 都有自己的 next_listen_num_，如果当前期望监听的比已经写入的小直接读，不需要等待条件变量。如果当前期望监听的比已经写入的大，则等待条件变量。

```cpp
bool ConditionNotifier::Listen(int timeout_ms, ReadableInfo* info) {
  if (info == nullptr) {
    AERROR << "info nullptr.";
    return false;
  }

  if (is_shutdown_.load()) {
    ADEBUG << "notifier is shutdown.";
    return false;
  }

  std::unique_lock<std::mutex> lck(indicator_->mtx);
  if (next_listen_num_ >= indicator_->written_info_num) {
    uint64_t target = next_listen_num_;
    if (!indicator_->cv.wait_for(
            lck, std::chrono::milliseconds(timeout_ms), [target, this]() {
              return this->indicator_->written_info_num > target ||
                     this->is_shutdown_.load();
            })) {
      ADEBUG << "timeout";
      return false;
    }

    if (is_shutdown_.load()) {
      AINFO << "notifier is shutdown.";
      return false;
    }
  }

  if (next_listen_num_ == 0) {
    next_listen_num_ = indicator_->written_info_num - 1;
  }

  auto idx = next_listen_num_ % kBufLength;
  *info = indicator_->infos[idx];
  next_listen_num_ += 1;

  return true;
}
```

## 2 ShmDispatcher 中 不同channel的idx：Segments和previous_indexes_

ShmDispatcher这个单例类中 的segments_(std::unordered_map<uint64_t, SegmentPtr>) 装有对应channel 的共享内存地址。

std::unordered_map<uint64_t, uint32_t> previous_indexes_;是每个channel的上一条消息的idx。

当监听到有消息来临时，会先判断本进程中是否监听了该话题，没有就不做处理。监听话题的idx也做了 **相同/之前/跳变** 等异常idx的打印。获取到idx后找到对应的**segment**去尝试给该**block**加一个读锁。block中有读写锁的实现，**原子变量**通过**compare_exchange_weak**实现的**读写锁**。读到消息后，先把MessageInfo( 发送者标识sender_id; 序列号seq_num;) DeserializeFrom 反序列化，成功后调用onMessage，在这里反序列化msg数据，(**把共享内存中的数据拷贝到本进程了**)。执行data::DataDispatcher\<MessageT>::Instance()->Dispatch回调派发数据。

### Block 中的原子变量读写锁

```cpp
const int32_t Block::kRWLockFree = 0;
const int32_t Block::kWriteExclusive = -1;
const int32_t Block::kMaxTryLockTimes = 5;
Block::Block() : msg_size_(0), msg_info_size_(0) {}
Block::~Block() {}

bool Block::TryLockForWrite() {
  int32_t rw_lock_free = kRWLockFree;
  if (!lock_num_.compare_exchange_weak(rw_lock_free, kWriteExclusive,
                                       std::memory_order_acq_rel,
                                       std::memory_order_relaxed)) {
    ADEBUG << "lock num: " << lock_num_.load();
    return false;
  }
  return true;
}

bool Block::TryLockForRead() {
  int32_t lock_num = lock_num_.load();
  if (lock_num < kRWLockFree) {
    AINFO << "block is being written.";
    return false;
  }

  int32_t try_times = 0;
  while (!lock_num_.compare_exchange_weak(lock_num, lock_num + 1,
                                          std::memory_order_acq_rel,
                                          std::memory_order_relaxed)) {
    ++try_times;
    if (try_times == kMaxTryLockTimes) {
      AINFO << "fail to add read lock num, curr num: " << lock_num;
      return false;
    }

    lock_num = lock_num_.load();
    if (lock_num < kRWLockFree) {
      AINFO << "block is being written.";
      return false;
    }
  }
  return true;
}
void Block::ReleaseWriteLock() { lock_num_.fetch_add(1); }
void Block::ReleaseReadLock() { lock_num_.fetch_sub(1); }
```



```cpp
void ShmDispatcher::ThreadFunc() {
  ReadableInfo readable_info;
  while (!is_shutdown_.load()) {
    if (!notifier_->Listen(100, &readable_info)) {
      ADEBUG << "listen failed.";
      continue;
    }

    uint64_t host_id = readable_info.host_id();
    if (host_id != host_id_) {
      ADEBUG << "shm readable info from other host.";
      continue;
    }

    uint64_t channel_id = readable_info.channel_id();
    uint32_t block_index = readable_info.block_index();

    {
      ReadLockGuard<AtomicRWLock> lock(segments_lock_);
      if (segments_.count(channel_id) == 0) {//并没有关注这个话题，即使有了消息，也不用去读
        continue;
      }
      // check block index
      if (previous_indexes_.count(channel_id) == 0) {
        previous_indexes_[channel_id] = UINT32_MAX;
      }
      uint32_t& previous_index = previous_indexes_[channel_id];
      if (block_index != 0 && previous_index != UINT32_MAX) {
        if (block_index == previous_index) {
          ADEBUG << "Receive SAME index " << block_index << " of channel "
                 << channel_id;
        } else if (block_index < previous_index) {
          ADEBUG << "Receive PREVIOUS message. last: " << previous_index
                 << ", now: " << block_index;
        } else if (block_index - previous_index > 1) {
          ADEBUG << "Receive JUMP message. last: " << previous_index
                 << ", now: " << block_index;
        }
      }
      previous_index = block_index;

      ReadMessage(channel_id, block_index);
    }
  }
}
```

## 3  CacheBuffer的idx： 订阅同一话题的不同reader的CacheBuffer的next_msg_index_

DataDispatcher\<MessageT>::Instance()->Dispatch，在reader的创建过程中会开辟一块由用户指定大小的`BufferType<T>(CacheBuffer)`和channel_id绑定，此时会往所有的buffer中派发消息(指针)，然后将协程设置为SetUpdateFlag更新状态，在 Processor::Run中获取下一个协程，查询到UpdateState后，协程设置为READY。协程去开始执行自己的routine，此时的TryFetch一定会成功，对应reader的next_msg_index_也会增加。如果drop了数据的话，在Fetch中也会记录日志，因为ringbuffer的size是自己指定的，要配置合适的buffer大小，或者reader的回调函数尽量异步的post任务，防止因为buffer的size过小错过数据。

```cpp
//派发消息，派发完毕后Notify
template <typename T>
bool DataDispatcher<T>::Dispatch(const uint64_t channel_id,
                                 const std::shared_ptr<T>& msg) {
  BufferVector* buffers = nullptr;
  if (apollo::cyber::IsShutdown()) {
    return false;
  }
  if (buffers_map_.Get(channel_id, &buffers)) {
    for (auto& buffer_wptr : *buffers) {
      if (auto buffer = buffer_wptr.lock()) {
        std::lock_guard<std::mutex> lock(buffer->Mutex());
        buffer->Fill(msg);//
      }
    }
  } else {
    return false;
  }
  return notifier_->Notify(channel_id);
}
```

每个处理单元做的就是轮询 就绪协程

```cpp
void Processor::Run() {
  tid_.store(static_cast<int>(syscall(SYS_gettid)));

  while (likely(running_)) {
    if (likely(context_ != nullptr)) {
      auto croutine = context_->NextRoutine();//获取下一个协程
      if (croutine) {
        croutine->Resume();//执行
        croutine->Release();
      } else {
        context_->Wait();//等待可执行的协程产生
      }
    } else {
      std::unique_lock<std::mutex> lk(mtx_ctx_);
      cv_ctx_.wait_for(lk, std::chrono::milliseconds(10));
    }
  }
}

```

获取下一个协程采取的策略

```cpp
std::shared_ptr<CRoutine> ClassicContext::NextRoutine() {
  if (unlikely(stop_)) {
    return nullptr;
  }

  for (int i = MAX_PRIO - 1; i >= 0; --i) { // 在自己绑定的协程组(group_name)中，从高到低去协程组的优先级队列中取任务 19是最高优先级 0 最低
    ReadLockGuard<AtomicRWLock> lk(rq_locks_[group_name_].at(i));
    for (auto& cr : cr_group_[group_name_].at(i)) { // 从相同优先级队列中 遍历协程任务
      if (!cr->Acquire()) {
        continue;
      }

      //对于DataVistor来说，有数据后，TryFetch成功才会置为就绪态，可以执行读回调
      if (cr->UpdateState() == RoutineState::READY) {
        PerfEventCache::Instance()->AddSchedEvent(SchedPerf::NEXT_RT, cr->id(),
                                                  cr->processor_id());
        return cr;
      }

      if (unlikely(cr->state() == RoutineState::SLEEP)) {
        if (!need_sleep_ || wake_time_ > cr->wake_time()) { //取较小的时间，醒来后肯定有任务能执行
          need_sleep_ = true;
          wake_time_ = cr->wake_time();
        }
      }

      cr->Release();
    }
  }

  return nullptr;
}
```

协程的routine函数，就是去取数据，取完之后设置为等待状态，yield

```cpp
  factory.create_routine = [=]() {
    return [=]() {
      std::shared_ptr<M0> msg;
      for (;;) {
        CRoutine::GetCurrentRoutine()->set_state(RoutineState::DATA_WAIT);
        if (dv->TryFetch(msg)) {
          f(msg);
          CRoutine::Yield(RoutineState::READY);
        } else {
          CRoutine::Yield();
        }
      }
    };
  };
```

Fetch中的drop记录

```cpp
bool ChannelBuffer<T>::Fetch(uint64_t* index,
                             std::shared_ptr<T>& m) {  // NOLINT
  std::lock_guard<std::mutex> lock(buffer_->Mutex());
  if (buffer_->Empty()) {
    return false;
  }

  if (*index == 0) { //如果还没初始化
    *index = buffer_->Tail(); //读取最新的cache
  } else if (*index == buffer_->Tail() + 1) { //现有的cache已经读过了，且还没有新数据到来，
    return false;
  } else if (*index < buffer_->Head()) { //太久没读，要读的已经被cache覆盖了，就读最新的
    auto interval = buffer_->Tail() - *index;
    AWARN << "channel[" << GlobalData::GetChannelById(channel_id_) << "] "
          << "read buffer overflow, drop_message[" << interval << "] pre_index["
          << *index << "] current_index[" << buffer_->Tail() << "] ";
    *index = buffer_->Tail();
  }//否则就是还在cache的范围之内，读对应的数据
  m = buffer_->at(*index);
  return true;
}
```

### CacheBuffer  环形队列

```cpp
template <typename T>
class CacheBuffer {
 public:
  using value_type = T;
  using size_type = std::size_t;

  explicit CacheBuffer(uint32_t size) {
    capacity_ = size + 1;
    buffer_.resize(capacity_);
  }

  CacheBuffer(const CacheBuffer& rhs) {
    std::lock_guard<std::mutex> lg(rhs.mutex_);
    head_ = rhs.head_;
    tail_ = rhs.tail_;
    buffer_ = rhs.buffer_;
    capacity_ = rhs.capacity_;
  }

  T& operator[](const uint64_t& pos) { return buffer_[GetIndex(pos)]; }
  const T& at(const uint64_t& pos) const { return buffer_[GetIndex(pos)]; }

  uint64_t Head() const { return head_ + 1; }
  uint64_t Tail() const { return tail_; }
  uint64_t Size() const { return tail_ - head_; }

  const T& Front() const { return buffer_[GetIndex(head_ + 1)]; }
  const T& Back() const { return buffer_[GetIndex(tail_)]; }

  bool Empty() const { return tail_ == 0; }
  bool Full() const { return capacity_ - 1 == tail_ - head_; }

  void Fill(const T& value) {
    if (Full()) {
      buffer_[GetIndex(head_)] = value;
      ++head_;
      ++tail_;
    } else {
      buffer_[GetIndex(tail_ + 1)] = value;
      ++tail_;
    }
  }

  std::mutex& Mutex() { return mutex_; }

 private:
  CacheBuffer& operator=(const CacheBuffer& other) = delete;
  uint64_t GetIndex(const uint64_t& pos) const { return pos % capacity_; }

  uint64_t head_ = 0;
  uint64_t tail_ = 0;
  uint64_t capacity_ = 0;
  std::vector<T> buffer_;
  mutable std::mutex mutex_;
};
```





```cpp
  bool TryFetch(std::shared_ptr<M0>& m0) {  // NOLINT  //获取cache中的数据
    if (buffer_.Fetch(&next_msg_index_, m0)) {// 向buffer中取数据
      next_msg_index_++;
      return true;
    }
    return false;
  }
```

